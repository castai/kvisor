controls: ""
version: aks-1.6.0
id: 3
text: Worker Nodes
type: node
groups:
    - id: "3.1"
      text: Worker Node Configuration Files
      checks:
        - id: 3.1.1
          text: Ensure that the kubeconfig file permissions are set to 644 or more restrictive
          audit: '/bin/sh -c ''if test -e $kubeletkubeconfig; then stat -c permissions=%a $kubeletkubeconfig; fi'' '
          type: automated
          tests:
            test_items:
                - flag: permissions
                  compare:
                    op: bitmask
                    value: "644"
          remediation: |-
            Run the below command (based on the file location on your system) on the each worker
            node. For example,
            ```
            chmod 644 <kubeconfig file>
            ```
          scored: false
        - id: 3.1.2
          text: Ensure that the kubelet kubeconfig file ownership is set to root:root
          audit: '/bin/sh -c ''if test -e $kubeletkubeconfig; then stat -c %U:%G $kubeletkubeconfig; fi'' '
          type: automated
          tests:
            test_items:
                - flag: root:root
          remediation: |-
            Run the below command (based on the file location on your system) on each worker node. For example,

            ```
            chown root:root <proxy kubeconfig file>
            ```
          scored: false
        - id: 3.1.3
          text: Ensure that the azure.json file has permissions set to 644 or more restrictive
          audit: '/bin/sh -c ''if test -e $kubernetesazurejson; then stat -c permissions=%a $kubernetesazurejson; fi'' '
          type: automated
          tests:
            test_items:
                - flag: permissions
                  compare:
                    op: bitmask
                    value: "644"
          remediation: |-
            Run the following command (using the config file location identified in the Audit step)

            ```
            chmod 644 /etc/kubernetes/azure.json
            ```
          scored: false
        - id: 3.1.4
          text: Ensure that the azure.json file ownership is set to root:root
          audit: '/bin/sh -c ''if test -e $kubernetesazurejson; then stat -c %U:%G $kubernetesazurejson; fi'' '
          type: automated
          tests:
            test_items:
                - flag: root:root
          remediation: |-
            Run the following command (using the config file location identified in the Audit step)

            ```
            chown root:root /etc/kubernetes/azure.json
            ```
          scored: false
    - id: "3.2"
      text: Kubelet
      checks:
        - id: 3.2.1
          text: Ensure that the --anonymous-auth argument is set to false
          audit: /bin/ps -fC $kubeletbin
          audit_config: /bin/cat $kubeletconf
          type: automated
          tests:
            test_items:
                - flag: --anonymous-auth
                  path: '{.authentication.anonymous.enabled}'
                  compare:
                    op: eq
                    value: "false"
          remediation: |-
            **Remediation Method 1:**

            If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to false

            ```
            "anonymous": "enabled": false
            ```

            **Remediation Method 2:**

            If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.

            ```
            --anonymous-auth=false
            ```

            **Remediation Method 3:**

            If using the api configz endpoint consider searching for the status of `"authentication.*anonymous":{"enabled":false}"` by extracting the live configuration from the nodes running kubelet.

            **See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes
            ```
            kubectl proxy --port=8001 &

            export HOSTNAME_PORT=localhost:8001 (example host and port number)
            export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from "kubectl get nodes")

            curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
            ```

            **For all three remediations:**
            Based on your system, restart the `kubelet` service and check status

            ```
            systemctl daemon-reload
            systemctl restart kubelet.service
            systemctl status kubelet -l
            ```
          scored: true
        - id: 3.2.2
          text: Ensure that the --authorization-mode argument is not set to AlwaysAllow
          audit: /bin/ps -fC $kubeletbin
          audit_config: /bin/cat $kubeletconf
          type: automated
          tests:
            test_items:
                - flag: --authorization-mode
                  path: '{.authorization.mode}'
                  compare:
                    op: nothave
                    value: AlwaysAllow
          remediation: "**Remediation Method 1:**\n\nIf modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to false\n\n ```\n\"authentication\"... \"webhook\":{\"enabled\":true\n``` \n\n**Remediation Method 2:**\n\nIf using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n```\n--authorization-mode=Webhook\n```\n\n**Remediation Method 3:**\n\nIf using the api configz endpoint consider searching for the status of `\"authentication.*webhook\":{\"enabled\":true\"` by extracting the live configuration from the nodes running kubelet.\n\n**See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n```\nkubectl proxy --port=8001 &\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n```\n\n**For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n```"
          scored: true
        - id: 3.2.3
          text: Ensure that the --client-ca-file argument is set as appropriate
          audit: /bin/ps -fC $kubeletbin
          audit_config: /bin/cat $kubeletconf
          type: automated
          tests:
            test_items:
                - flag: --client-ca-file
                  path: '{.authentication.x509.clientCAFile}'
                  set: true
          remediation: "**Remediation Method 1:**\n\nIf modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to false\n\n```\n\"authentication\": { \"x509\": {\"clientCAFile:\" to the location of the client CA file.\n``` \n\n**Remediation Method 2:**\n\nIf using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n```\n--client-ca-file=<path/to/client-ca-file>\n```\n\n**Remediation Method 3:**\n\nIf using the api configz endpoint consider searching for the status of `\"authentication.*x509\":(\"clientCAFile\":\"/etc/kubernetes/pki/ca.crt\"` by extracting the live configuration from the nodes running kubelet.\n\n**See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n```\nkubectl proxy --port=8001 &\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n```\n\n**For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n```\n```"
          scored: true
        - id: 3.2.4
          text: Ensure that the --read-only-port is secured
          audit: /bin/ps -fC $kubeletbin
          audit_config: /bin/cat $kubeletconf
          type: automated
          tests:
            test_items:
                - flag: --read-only-port
                  path: '{.readOnlyPort}'
                  set: true
                  compare:
                    op: eq
                    value: "0"
          remediation: |-
            If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to false

            ```
            readOnlyPort to 0
            ```

            If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.

            ```
            --read-only-port=0
            ```

            For all remediations:
            Based on your system, restart the `kubelet` service and check status

            ```
            systemctl daemon-reload
            systemctl restart kubelet.service
            systemctl status kubelet -l
            ```
          scored: false
        - id: 3.2.5
          text: Ensure that the --streaming-connection-idle-timeout argument is not set to 0
          audit: /bin/ps -fC $kubeletbin
          audit_config: /bin/cat $kubeletconf
          type: automated
          tests:
            test_items:
                - flag: --streaming-connection-idle-timeout
                  path: '{.streamingConnectionIdleTimeout}'
                  set: true
                  compare:
                    op: noteq
                    value: "0"
                - flag: --streaming-connection-idle-timeout
                  path: '{.streamingConnectionIdleTimeout}'
            bin_op: or
          remediation: |-
            **Remediation Method 1:**

            If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to a non-zero value in the format of #h#m#s

            ```
            "streamingConnectionIdleTimeout": "4h0m0s"
            ```

            You should ensure that the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` does not specify a `--streaming-connection-idle-timeout` argument because it would override the Kubelet config file.

            **Remediation Method 2:**

            If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.

            ```
            --streaming-connection-idle-timeout=4h0m0s
            ```

            **Remediation Method 3:**

            If using the api configz endpoint consider searching for the status of `"streamingConnectionIdleTimeout":` by extracting the live configuration from the nodes running kubelet.

            **See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes
            ```
            kubectl proxy --port=8001 &

            export HOSTNAME_PORT=localhost:8001 (example host and port number)
            export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from "kubectl get nodes")

            curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
            ```

            **For all three remediations:**
            Based on your system, restart the `kubelet` service and check status

            ```
            systemctl daemon-reload
            systemctl restart kubelet.service
            systemctl status kubelet -l
            ```
          scored: true
        - id: 3.2.6
          text: Ensure that the --make-iptables-util-chains argument is set to true
          audit: /bin/ps -fC $kubeletbin
          audit_config: /bin/cat $kubeletconf
          type: automated
          tests:
            test_items:
                - flag: --make-iptables-util-chains
                  path: '{.makeIPTablesUtilChains}'
                  set: true
                  compare:
                    op: eq
                    value: "true"
                - flag: --make-iptables-util-chains
                  path: '{.makeIPTablesUtilChains}'
            bin_op: or
          remediation: |-
            **Remediation Method 1:**

            If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to false

            ```
            "makeIPTablesUtilChains": true
            ```

            **Remediation Method 2:**

            If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.

            ```
            --make-iptables-util-chains:true
            ```

            **Remediation Method 3:**

            If using the api configz endpoint consider searching for the status of `"makeIPTablesUtilChains": true` by extracting the live configuration from the nodes running kubelet.

            **See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes
            ```
            kubectl proxy --port=8001 &

            export HOSTNAME_PORT=localhost:8001 (example host and port number)
            export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from "kubectl get nodes")

            curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
            ```

            **For all three remediations:**
            Based on your system, restart the `kubelet` service and check status

            ```
            systemctl daemon-reload
            systemctl restart kubelet.service
            systemctl status kubelet -l
          scored: true
        - id: 3.2.7
          text: Ensure that the --eventRecordQPS argument is set to 0 or a level which ensures appropriate event capture
          audit: "/bin/ps -fC $kubeletbin"
          audit_config: "/bin/cat $kubeletconf"
          tests:
            test_items:
              - flag: --event-qps
                path: '{.eventRecordQPS}'
                set: true
                compare:
                  op: eq
                  value: 0
          remediation: |-
            **Remediation Method 1:**

            If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to 5 or a value greater or equal to 0

            ```
            "eventRecordQPS": 5
            ```

            Check that `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` does not define an executable argument for `eventRecordQPS` because this would override your Kubelet config.

            **Remediation Method 2:**

            If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.

            ```
            --eventRecordQPS=5
            ```

            **Remediation Method 3:**

            If using the api configz endpoint consider searching for the status of `"eventRecordQPS"` by extracting the live configuration from the nodes running kubelet.

            **See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes
            ```
            kubectl proxy --port=8001 &

            export HOSTNAME_PORT=localhost:8001 (example host and port number)
            export NODE_NAME=ip-192.168.31.226.aks.internal (example node name from "kubectl get nodes")

            curl -sSL "http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz"
            ```

            **For all three remediations:**
            Based on your system, restart the `kubelet` service and check status

            ```
            systemctl daemon-reload
            systemctl restart kubelet.service
            systemctl status kubelet -l
            ```
          scored: false
        - id: 3.2.8
          text: Ensure that the --rotate-certificates argument is not set to false
          audit: "/bin/ps -fC $kubeletbin"
          audit_config: "/bin/cat $kubeletconf"
          tests:
            test_items:
              - flag: --rotate-certificates
                path: '{.rotateCertificates}'
                set: true
                compare:
                  op: eq
                  value: true
              - flag: --rotate-certificates
                path: '{.rotateCertificates}'
                set: false
            bin_op: or
          remediation: |-
            **Remediation Method 1:**

            If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to true

            ```
            "RotateCertificate":true
            ```

            Additionally, ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not set the --RotateCertificate executable argument to false because this would override the Kubelet config file.

            **Remediation Method 2:**

            If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.

            ```
            --RotateCertificate=true
            ```
          scored: true
        - id: 3.2.9
          text: Ensure that the RotateKubeletServerCertificate argument is set to true
          audit: "/bin/ps -fC $kubeletbin"
          audit_config: "/bin/cat $kubeletconf"
          tests:
            test_items:
              - flag: RotateKubeletServerCertificate
                path: '{.featureGates.RotateKubeletServerCertificate}'
                set: true
                compare:
                  op: eq
                  value: true
          remediation: "**Remediation Method 1:**\n\nIf modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to true\n\n```\n\"RotateKubeletServerCertificate\":true\n```\n\n**Remediation Method 2:**\n\nIf using a Kubelet config file, edit the file to set `RotateKubeletServerCertificate to true`. \n\nIf using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n```\n--rotate-kubelet-server-certificate=true\n```\n\n**Remediation Method 3:**\n\nIf using the api configz endpoint consider searching for the status of `\"RotateKubeletServerCertificate\":` by extracting the live configuration from the nodes running kubelet.\n\n**See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n```\nkubectl proxy --port=8001 &\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n```\n\n**For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n```"
          scored: true
