{{- if and .Values.agent.enabled (dig "reliabilityMetrics" "enabled" false .Values.agent) (dig "reliabilityMetrics" "collector" "enabled" false .Values.agent) }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "kvisor.agent.fullname" . }}-otel-collector
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "kvisor.agent.labels" . | nindent 4 }}
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          http:
            endpoint: "0.0.0.0:4318"

    processors:
      # Keep only golden signal metrics from OBI — drop everything else.
      # Covers: Latency (server+client), Errors (via status_code attrs),
      #         Traffic (histogram _count for RPS, body sizes for bytes).
      filter/golden-signals:
        metrics:
          include:
            match_type: regexp
            metric_names:
              # HTTP server latency (RED)
              - "http\\.server\\.request\\.duration"
              # HTTP client dependency latency (outbound calls)
              - "http\\.client\\.request\\.duration"
              # gRPC server latency
              - "rpc\\.server\\.duration"
              # gRPC client dependency latency
              - "rpc\\.client\\.duration"
              # Database client latency (Redis, SQL, etc. — OBI uses OTel semconv
              # db.client.operation.duration with db.system attribute for differentiation)
              - "db\\.client\\.operation\\.duration"
              # Messaging latency
              - "messaging\\.publish\\.duration"
              - "messaging\\.process\\.duration"
              # HTTP request/response body sizes (byte throughput)
              - "http\\.server\\.request\\.body\\.size"
              - "http\\.server\\.response\\.body\\.size"
              - "http\\.client\\.request\\.body\\.size"
              - "http\\.client\\.response\\.body\\.size"
              # OBI network flow counters
              - "obi\\.network\\.flow\\.bytes"
              - "obi\\.network\\.inter\\.zone\\.bytes"

      # Dedup guard: K8s infrastructure metrics are owned by the controller
      # collector's k8s_cluster receiver. Drop them here to prevent duplicates
      # if a future Prometheus receiver scrapes targets that emit kube_* or k8s.* metrics.
      filter/drop-k8s:
        metrics:
          exclude:
            match_type: regexp
            metric_names:
              - "k8s\\..*"
              - "kube_.*"

      # Strip high-cardinality attributes to control Prometheus series count.
      # Keep only what WOOP and SLO calculations need.
      transform/cardinality:
        metric_statements:
          - context: datapoint
            statements:
              # Remove high-cardinality URL/path attributes
              - delete_key(attributes, "url.full")
              - delete_key(attributes, "url.path")
              - delete_key(attributes, "server.address")
              - delete_key(attributes, "server.port")
              - delete_key(attributes, "client.address")
              - delete_key(attributes, "client.port")
              - delete_key(attributes, "network.peer.address")
              - delete_key(attributes, "network.peer.port")
              - delete_key(attributes, "user_agent.original")
              # Sanitise messaging.destination.name — OBI eBPF probes may
              # read past the Go string boundary, appending garbage bytes.
              # Truncate at the first non-printable / obviously invalid char.
              - replace_pattern(attributes["messaging.destination.name"], "^([a-zA-Z0-9._-]+).*", "$$1")
              # Keep: http.request.method, http.response.status_code,
              #        service.name, service.instance.id,
              #        rpc.method, rpc.service, rpc.grpc.status_code,
              #        db.system.name, db.operation.name, error.type,
              #        messaging.system, messaging.destination.name

      # Batch before export to reduce scrape overhead.
      batch:
        send_batch_size: 1024
        timeout: 10s

      memory_limiter:
        check_interval: 5s
        limit_mib: 100
        spike_limit_mib: 25

    exporters:
      prometheus:
        endpoint: "0.0.0.0:{{ .Values.agent.reliabilityMetrics.collector.prometheusPort }}"
        resource_to_telemetry_conversion:
          enabled: true
        metric_expiration: 5m

{{- if dig "reliabilityMetrics" "collector" "clickhouseExporter" "enabled" false .Values.agent }}
      clickhouse:
        endpoint: tcp://${env:OTEL_CLICKHOUSE_CONFIG_ADDRESS}?dial_timeout=10s&compress=lz4
        database: otel
        username: ${env:OTEL_CLICKHOUSE_CONFIG_USERNAME}
        password: ${env:OTEL_CLICKHOUSE_CONFIG_PASSWORD}
        ttl: 72h
        timeout: 10s
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
{{- end }}

    extensions:
      health_check:
        endpoint: "0.0.0.0:13133"

    service:
      telemetry:
        metrics:
          readers:
            - pull:
                exporter:
                  prometheus:
                    host: "0.0.0.0"
                    port: 8888
      extensions: [health_check]
      pipelines:
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, filter/drop-k8s, filter/golden-signals, transform/cardinality, batch]
          exporters: [prometheus{{- if dig "reliabilityMetrics" "collector" "clickhouseExporter" "enabled" false .Values.agent }}, clickhouse{{- end }}]
{{- end }}
---
{{- if and .Values.controller.enabled (dig "reliabilityMetrics" "enabled" false .Values.controller) (dig "reliabilityMetrics" "collector" "enabled" false .Values.controller) }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "kvisor.controller.fullname" . }}-otel-collector
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "kvisor.controller.labels" . | nindent 4 }}
data:
  config.yaml: |
    receivers:
      # Watches K8s API server for cluster state — KSM-equivalent metrics.
      # Must run as single instance (not per-node) to avoid duplicate data.
      k8s_cluster:
        auth_type: serviceAccount
        collection_interval: 30s
        node_conditions_to_report:
          - Ready
        allocatable_types_to_report:
          - cpu
          - memory

    processors:
      # Keep only reliability-relevant K8s metrics — drop everything else.
      filter/reliability:
        metrics:
          include:
            match_type: regexp
            metric_names:
              # Pod health
              - "k8s\\.pod\\.phase"
              # Container health
              - "k8s\\.container\\.restarts"
              - "k8s\\.container\\.ready"
              # Deployment availability
              - "k8s\\.deployment\\.desired"
              - "k8s\\.deployment\\.available"
              # ReplicaSet health
              - "k8s\\.replicaset\\.desired"
              - "k8s\\.replicaset\\.available"
              # DaemonSet health
              - "k8s\\.daemonset\\.desired_scheduled"
              - "k8s\\.daemonset\\.current_scheduled"
              - "k8s\\.daemonset\\.ready"
              - "k8s\\.daemonset\\.misscheduled"
              # StatefulSet health
              - "k8s\\.statefulset\\.desired_pods"
              - "k8s\\.statefulset\\.ready_pods"
              - "k8s\\.statefulset\\.current_pods"
              - "k8s\\.statefulset\\.updated_pods"
              # Job / CronJob health
              - "k8s\\.job\\.active_pods"
              - "k8s\\.job\\.failed_pods"
              - "k8s\\.job\\.successful_pods"
              - "k8s\\.cronjob\\.active_jobs"
              # HPA scaling pressure
              - "k8s\\.hpa\\.current_replicas"
              - "k8s\\.hpa\\.desired_replicas"
              - "k8s\\.hpa\\.max_replicas"
              - "k8s\\.hpa\\.min_replicas"
              # Node conditions
              - "k8s\\.node\\.condition_ready"
              # Resource requests and limits
              - "k8s\\.container\\.cpu_request"
              - "k8s\\.container\\.cpu_limit"
              - "k8s\\.container\\.memory_request"
              - "k8s\\.container\\.memory_limit"

      batch:
        send_batch_size: 1024
        timeout: 10s

      memory_limiter:
        check_interval: 5s
        limit_mib: 200
        spike_limit_mib: 50

    exporters:
      prometheus:
        endpoint: "0.0.0.0:{{ .Values.controller.reliabilityMetrics.collector.prometheusPort }}"
        resource_to_telemetry_conversion:
          enabled: true
        metric_expiration: 5m

{{- if dig "reliabilityMetrics" "collector" "clickhouseExporter" "enabled" false .Values.controller }}
      clickhouse:
        endpoint: tcp://${env:OTEL_CLICKHOUSE_CONFIG_ADDRESS}?dial_timeout=10s&compress=lz4
        database: otel
        username: ${env:OTEL_CLICKHOUSE_CONFIG_USERNAME}
        password: ${env:OTEL_CLICKHOUSE_CONFIG_PASSWORD}
        ttl: 72h
        timeout: 10s
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
{{- end }}

    extensions:
      health_check:
        endpoint: "0.0.0.0:13134"

    service:
      telemetry:
        metrics:
          readers:
            - pull:
                exporter:
                  prometheus:
                    host: "0.0.0.0"
                    port: 8889
      extensions: [health_check]
      pipelines:
        metrics:
          receivers: [k8s_cluster]
          processors: [memory_limiter, filter/reliability, batch]
          exporters: [prometheus{{- if dig "reliabilityMetrics" "collector" "clickhouseExporter" "enabled" false .Values.controller }}, clickhouse{{- end }}]
{{- end }}
