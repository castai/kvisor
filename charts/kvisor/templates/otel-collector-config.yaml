{{- if and .Values.agent.enabled (dig "reliabilityMetrics" "enabled" false .Values.agent) (dig "reliabilityMetrics" "collector" "enabled" false .Values.agent) }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "kvisor.agent.fullname" . }}-otel-collector
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "kvisor.agent.labels" . | nindent 4 }}
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          http:
            endpoint: "0.0.0.0:4318"

    processors:
      # ─────────────────────────────────────────────────────────────────────
      # METRIC FILTER — keep only golden signal metrics from OBI
      # ─────────────────────────────────────────────────────────────────────
      # Covers: Latency (server+client), Errors (via status_code attrs),
      #         Traffic (histogram _count for RPS).
      # IMPORTANT: Runs BEFORE cumulativetodelta — fewer series = less
      # delta state memory (each unique series costs ~1KB of state).
      filter/golden-signals:
        metrics:
          include:
            match_type: regexp
            metric_names:
              # HTTP server latency (RED)
              - "http\\.server\\.request\\.duration"
              # HTTP client dependency latency (outbound calls)
              - "http\\.client\\.request\\.duration"
              # gRPC server latency
              - "rpc\\.server\\.duration"
              # gRPC client dependency latency
              - "rpc\\.client\\.duration"
              # Database client latency (Redis, SQL, etc. — OBI uses OTel semconv
              # db.client.operation.duration with db.system attribute for differentiation)
              - "db\\.client\\.operation\\.duration"
              # Messaging latency
              - "messaging\\.publish\\.duration"
              - "messaging\\.process\\.duration"

      # Dedup guard: K8s infrastructure metrics are owned by the controller
      # collector's k8s_cluster receiver. Drop them here to prevent duplicates
      # if a future Prometheus receiver scrapes targets that emit kube_* or k8s.* metrics.
      filter/drop-k8s:
        metrics:
          exclude:
            match_type: regexp
            metric_names:
              - "k8s\\..*"
              - "kube_.*"

      # ─────────────────────────────────────────────────────────────────────
      # CUMULATIVE → DELTA (ClickHouse pipeline ONLY)
      # ─────────────────────────────────────────────────────────────────────
      # OBI emits CUMULATIVE temporality (AggregationTemporality=2).
      # Without conversion, histogram Count/Sum/BucketCounts are ever-growing
      # since process start — sum(Count) in Silver MVs would produce wildly
      # inflated RPS, error rates, and latency averages.
      #
      # After conversion:
      #   Count       = requests in THIS interval (not since process start)
      #   Sum         = total duration in THIS interval
      #   BucketCounts = distribution in THIS interval
      #   AggregationTemporality changes from 2 → 1 (DELTA)
      #
      # This makes sum(Count), sum(Sum), sumForEach(BucketCounts) in
      # Silver MVs produce correct aggregates.
      #
      # NOT in the Prometheus pipeline — Prometheus expects cumulative and
      # applies rate()/increase() at query time.
      #
      # Memory: ~1KB per unique series. With filter/golden-signals upstream,
      # only OBI golden signal series consume state.
      #
      # On process restart: cumulativetodelta detects counter resets
      # (value going backwards) and emits a correct delta from the new
      # starting point.
      cumulativetodelta:
        include:
          match_type: ""
        max_staleness: 600s

      # ─────────────────────────────────────────────────────────────────────
      # CARDINALITY CONTROL — strip high-cardinality attributes
      # ─────────────────────────────────────────────────────────────────────
      # resource_to_telemetry_conversion promotes every resource attribute to
      # a Prometheus label — strip anything that creates per-pod or per-node
      # series multipliers with zero analytical value.
      #
      # Attributes that survive (used by Silver GROUP BY / Gold MVs):
      #   Datapoint:  http.request.method, http.response.status_code,
      #               rpc.method, rpc.service, rpc.grpc.status_code,
      #               db.system.name, db.operation.name, error.type,
      #               messaging.system, messaging.destination.name
      #   Resource:   service.name, k8s.namespace.name, k8s.deployment.name,
      #               k8s.daemonset.name, k8s.statefulset.name, k8s.node.name
      transform/cardinality:
        metric_statements:
          - context: datapoint
            statements:
              # Remove high-cardinality URL/path attributes
              - delete_key(attributes, "url.full")
              - delete_key(attributes, "url.path")
              - delete_key(attributes, "url.query")
              - delete_key(attributes, "http.route")
              - delete_key(attributes, "server.address")
              - delete_key(attributes, "server.port")
              - delete_key(attributes, "client.address")
              - delete_key(attributes, "client.port")
              - delete_key(attributes, "network.peer.address")
              - delete_key(attributes, "network.peer.port")
              - delete_key(attributes, "user_agent.original")
              # DB statement — can contain full SQL queries, extremely high cardinality
              - delete_key(attributes, "db.statement")
              # Messaging message ID — per-message, not useful for aggregates
              - delete_key(attributes, "messaging.message.id")
              # Sanitise messaging.destination.name — OBI eBPF probes may
              # read past the Go string boundary, appending garbage bytes.
              # Truncate at the first non-printable / obviously invalid char.
              - replace_pattern(attributes["messaging.destination.name"], "^([a-zA-Z0-9._-]+).*", "$$1")
              # Keep: http.request.method, http.response.status_code,
              #        rpc.method, rpc.service, rpc.grpc.status_code,
              #        db.system.name, db.operation.name, error.type,
              #        messaging.system, messaging.destination.name
          - context: resource
            statements:
              # Host/SDK noise — no analytical value
              - delete_key(attributes, "host.id")
              - delete_key(attributes, "host.name")
              - delete_key(attributes, "os.type")
              - delete_key(attributes, "telemetry.sdk.language")
              - delete_key(attributes, "telemetry.sdk.name")
              - delete_key(attributes, "telemetry.sdk.version")
              # Per-instance ID — unique per pod × process, O(pods×procs) cardinality
              - delete_key(attributes, "service.instance.id")
              # Per-pod K8s attributes — cardinality explosion on rollouts
              - delete_key(attributes, "k8s.pod.name")
              - delete_key(attributes, "k8s.pod.uid")
              - delete_key(attributes, "k8s.pod.start_time")
              - delete_key(attributes, "k8s.container.name")
              - delete_key(attributes, "k8s.replicaset.name")
              - delete_key(attributes, "k8s.cluster.name")
              - delete_key(attributes, "k8s.owner.name")
              - delete_key(attributes, "k8s.kind")
              # Keep: service.name, service.namespace,
              #        k8s.namespace.name, k8s.deployment.name,
              #        k8s.daemonset.name, k8s.statefulset.name,
              #        k8s.node.name

      # ─────────────────────────────────────────────────────────────────────
      # BATCH — Prometheus pipeline
      # ─────────────────────────────────────────────────────────────────────
      # Keep timeout low (1s) so Prometheus exporter counters increment
      # near-continuously — avoids step-function spikes in rate() queries.
      batch:
        send_batch_size: 1024
        timeout: 1s

{{- if dig "reliabilityMetrics" "collector" "clickhouseExporter" "enabled" false .Values.agent }}
      # ─────────────────────────────────────────────────────────────────────
      # BATCH/CLICKHOUSE — ClickHouse pipeline
      # ─────────────────────────────────────────────────────────────────────
      # Larger batches = fewer ClickHouse parts = less merge pressure.
      # 10K data points or 30s, whichever comes first.
      # Goal: keep active parts < 100 per table.
      batch/clickhouse:
        send_batch_size: 10000
        send_batch_max_size: 15000
        timeout: 30s
{{- end }}

      memory_limiter:
        check_interval: 5s
        limit_mib: 100
        spike_limit_mib: 25

    exporters:
      prometheus:
        endpoint: "0.0.0.0:{{ .Values.agent.reliabilityMetrics.collector.prometheusPort }}"
        resource_to_telemetry_conversion:
          enabled: true
        metric_expiration: 5m

{{- if dig "reliabilityMetrics" "collector" "clickhouseExporter" "enabled" false .Values.agent }}
      clickhouse:
        endpoint: tcp://${env:OTEL_CLICKHOUSE_CONFIG_ADDRESS}?dial_timeout=10s&compress=lz4
        database: otel
        username: ${env:OTEL_CLICKHOUSE_CONFIG_USERNAME}
        password: ${env:OTEL_CLICKHOUSE_CONFIG_PASSWORD}
        # Schema managed by us (ddl/001_medallion_schema.sql) — disable
        # auto-creation to prevent the exporter from creating tables with
        # its own schema and the redundant histogram_to_sum_mv.
        # create_schema: false
        # Let ClickHouse batch small inserts server-side. Combined with
        # batch/clickhouse processor, this gives two levels of batching:
        # collector-side (10K points) + CH-side (async insert buffer).
        async_insert: true
        timeout: 10s
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
{{- end }}

    extensions:
      health_check:
        endpoint: "0.0.0.0:13133"

    service:
      telemetry:
        metrics:
          readers:
            - pull:
                exporter:
                  prometheus:
                    host: "0.0.0.0"
                    port: 8888
      extensions: [health_check]
      pipelines:
        # ─────────────────────────────────────────────────────────────────
        # PROMETHEUS PIPELINE — cumulative temporality (no delta conversion)
        # ─────────────────────────────────────────────────────────────────
        # Prometheus expects cumulative counters and applies rate()/increase()
        # at query time. Do NOT add cumulativetodelta here.
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, filter/drop-k8s, filter/golden-signals, transform/cardinality, batch]
          exporters: [prometheus]
{{- if dig "reliabilityMetrics" "collector" "clickhouseExporter" "enabled" false .Values.agent }}
        # ─────────────────────────────────────────────────────────────────
        # CLICKHOUSE PIPELINE — delta temporality for Silver MVs
        # ─────────────────────────────────────────────────────────────────
        # Pipeline order matters:
        #   1. memory_limiter      — backpressure before anything else
        #   2. filter/drop-k8s     — dedup guard
        #   3. filter/golden-signals — drop unused metrics BEFORE delta
        #                              (less series = less delta state memory)
        #   4. cumulativetodelta   — cumulative→delta BEFORE cardinality strip
        #   5. transform/cardinality — remove high-cardinality attributes
        #   6. batch/clickhouse    — large batches for fewer CH parts
        #
        # CRITICAL: cumulativetodelta MUST be here. Without it, Silver MVs
        # sum(Count) produces inflated values (summing ever-growing
        # cumulative counters instead of per-interval deltas).
        metrics/clickhouse:
          receivers: [otlp]
          processors: [memory_limiter, filter/drop-k8s, filter/golden-signals, cumulativetodelta, transform/cardinality, batch/clickhouse]
          exporters: [clickhouse]
{{- end }}
{{- end }}
---
{{- if and .Values.controller.enabled (dig "reliabilityMetrics" "enabled" false .Values.controller) (dig "reliabilityMetrics" "collector" "enabled" false .Values.controller) }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "kvisor.controller.fullname" . }}-otel-collector
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "kvisor.controller.labels" . | nindent 4 }}
data:
  config.yaml: |
    receivers:
      # Watches K8s API server for cluster state — KSM-equivalent metrics.
      # Must run as single instance (not per-node) to avoid duplicate data.
      k8s_cluster:
        auth_type: serviceAccount
        collection_interval: 30s
        node_conditions_to_report:
          - Ready
        allocatable_types_to_report:
          - cpu
          - memory

    processors:
      # Keep only reliability-relevant K8s metrics — drop everything else.
      filter/reliability:
        metrics:
          include:
            match_type: regexp
            metric_names:
              # Pod health
              - "k8s\\.pod\\.phase"
              # Container health
              - "k8s\\.container\\.restarts"
              - "k8s\\.container\\.ready"
              # Deployment availability
              - "k8s\\.deployment\\.desired"
              - "k8s\\.deployment\\.available"
              # ReplicaSet health
              - "k8s\\.replicaset\\.desired"
              - "k8s\\.replicaset\\.available"
              # DaemonSet health
              - "k8s\\.daemonset\\.desired_scheduled"
              - "k8s\\.daemonset\\.current_scheduled"
              - "k8s\\.daemonset\\.ready"
              - "k8s\\.daemonset\\.misscheduled"
              # StatefulSet health
              - "k8s\\.statefulset\\.desired_pods"
              - "k8s\\.statefulset\\.ready_pods"
              - "k8s\\.statefulset\\.current_pods"
              - "k8s\\.statefulset\\.updated_pods"
              # Job / CronJob health
              - "k8s\\.job\\.active_pods"
              - "k8s\\.job\\.failed_pods"
              - "k8s\\.job\\.successful_pods"
              - "k8s\\.cronjob\\.active_jobs"
              # HPA scaling pressure
              - "k8s\\.hpa\\.current_replicas"
              - "k8s\\.hpa\\.desired_replicas"
              - "k8s\\.hpa\\.max_replicas"
              - "k8s\\.hpa\\.min_replicas"
              # Node conditions
              - "k8s\\.node\\.condition_ready"
              # Resource requests and limits
              - "k8s\\.container\\.cpu_request"
              - "k8s\\.container\\.cpu_limit"
              - "k8s\\.container\\.memory_request"
              - "k8s\\.container\\.memory_limit"

      batch:
        send_batch_size: 1024
        timeout: 1s

{{- if dig "reliabilityMetrics" "collector" "clickhouseExporter" "enabled" false .Values.controller }}
      # Larger batches for ClickHouse — controller volume is low (30s
      # collection interval), but larger batches still reduce parts count.
      batch/clickhouse:
        send_batch_size: 5000
        send_batch_max_size: 8000
        timeout: 30s
{{- end }}

      memory_limiter:
        check_interval: 5s
        limit_mib: 200
        spike_limit_mib: 50

    exporters:
      prometheus:
        endpoint: "0.0.0.0:{{ .Values.controller.reliabilityMetrics.collector.prometheusPort }}"
        resource_to_telemetry_conversion:
          enabled: true
        metric_expiration: 5m

{{- if dig "reliabilityMetrics" "collector" "clickhouseExporter" "enabled" false .Values.controller }}
      clickhouse:
        endpoint: tcp://${env:OTEL_CLICKHOUSE_CONFIG_ADDRESS}?dial_timeout=10s&compress=lz4
        database: otel
        username: ${env:OTEL_CLICKHOUSE_CONFIG_USERNAME}
        password: ${env:OTEL_CLICKHOUSE_CONFIG_PASSWORD}
        # Schema managed by us — disable auto-creation
        # create_schema: false
        async_insert: true
        timeout: 10s
        retry_on_failure:
          enabled: true
          initial_interval: 5s
          max_interval: 30s
          max_elapsed_time: 300s
{{- end }}

    extensions:
      health_check:
        endpoint: "0.0.0.0:13134"

    service:
      telemetry:
        metrics:
          readers:
            - pull:
                exporter:
                  prometheus:
                    host: "0.0.0.0"
                    port: 8889
      extensions: [health_check]
      pipelines:
        # ─────────────────────────────────────────────────────────────────
        # PROMETHEUS PIPELINE — KSM gauges for Prometheus
        # ─────────────────────────────────────────────────────────────────
        metrics:
          receivers: [k8s_cluster]
          processors: [memory_limiter, filter/reliability, batch]
          exporters: [prometheus]
{{- if dig "reliabilityMetrics" "collector" "clickhouseExporter" "enabled" false .Values.controller }}
        # ─────────────────────────────────────────────────────────────────
        # CLICKHOUSE PIPELINE — KSM gauges for Silver gauge table
        # ─────────────────────────────────────────────────────────────────
        # No cumulativetodelta needed — k8s_cluster emits gauges, not
        # cumulative counters. Separate pipeline only for batch tuning.
        metrics/clickhouse:
          receivers: [k8s_cluster]
          processors: [memory_limiter, filter/reliability, batch/clickhouse]
          exporters: [clickhouse]
{{- end }}
{{- end }}
