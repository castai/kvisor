castai:
  # grpcAddr: "kvisor.dev-master.cast.ai:443"
  apiKey: "test"
  clusterID: "ci"
  enabled: true

image:
  repository: kvisor
  tag: latest

commonLabels:
  team: sec

pyroscope:
  enabled: true

agent:
  enabled: true
  extraArgs:
    log-level: debug
    prom-metrics-export-interval: 10s
    file-hash-enricher-enabled: true
    signature-socks5-detection-enabled: true
    stats-enabled: true
    stats-scrape-interval: 5s
    stats-file-access-enabled: true
    storage-stats-enabled: true
    containers-refresh-interval: "5s"
    ebpf-events-enabled: true
    netflow-enabled: true
    netflow-export-interval: 5s
    process-tree-enabled: true
    ebpf-events-include-pod-labels: 'helm.sh/chart,app.kubernetes.io/name'
    ebpf-events-include-pod-annotations: 'cast.ai'
    data-batch-flush-interval: 5s
    # Uncomment if you want to test high frequency events ingestion to mock server.
    #ebpf-events-policy: "magic_write,sched_process_exec,sock_set_state,net_packet_dns_base,write,open,close,futex,epoll_wait"

  prometheusScrape:
    enabled: true

  containerSecurityContext:
    readOnlyRootFilesystem: false

  resources:
    requests:
     cpu: 10m
     memory: 128Mi
    limits:
     memory: 256Mi
  reliabilityMetrics:
    enabled: true
    image:
      repository: otel/ebpf-instrument
      tag: "v0.4.1"
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        memory: 1Gi
    openPorts: "80,443,8080,8443,8090,6379"
    env:
      # Increase internal Go channel buffer from default 10 to 500.
      # Fixes "subscriber channel is taking too long to respond" warnings
      # that cause span drops and metric under-counting on busy nodes.
      OTEL_EBPF_CHANNEL_BUFFER_LEN: "500"
      # Reduce metrics export interval from default 60s to 15s.
      # Flushes histogram aggregations 4x more often, reducing backpressure
      # on the spanNameAggregatedMetrics pipeline stage.
      OTEL_EBPF_METRICS_INTERVAL: "15s"
      # Send eBPF events immediately instead of batching — reduces burst
      # size hitting the Go pipeline during high-throughput periods.
      OTEL_EBPF_BPF_HIGH_REQUEST_VOLUME: "true"
    containerSecurityContext: { }
    collector:
      enabled: true
      clickhouseExporter:
        enabled: false
      image:
        repository: kvisor-collector
        tag: latest
      args:
        - "run"
        - "--otlp-http-endpoint=0.0.0.0:4318"
        - "--metrics-exporter-port=9400"
        - "--health-check-endpoint=0.0.0.0:13133"
      resources:
        requests:
          memory: 64Mi
        limits:
          memory: 128Mi
      prometheusPort: 9400
      podMonitorLabels:
        release: kube-prometheus-stack

controller:
  enabled: true
  replicas: 1
  extraArgs:
    log-level: debug
    prom-metrics-export-interval: 10s
    image-scan-enabled: true
    image-scan-interval: 5s
    image-scan-init-delay: 5s
    kube-linter-enabled: true
    kube-linter-scan-interval: 5s

  extraEnv:
    SCANNERS_IMAGE: kvisor-scanners

  containerSecurityContext:
    readOnlyRootFilesystem: false
  securityContext:
    runAsNonRoot: false

  prometheusScrape:
    enabled: true
  reliabilityMetrics:
    enabled: true
    image:
      repository: otel/ebpf-instrument
      tag: "v0.4.1"
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        memory: 1Gi
    openPorts: "80,443,8080,8443,8090,6379"
    env:
      # Increase internal Go channel buffer from default 10 to 500.
      # Fixes "subscriber channel is taking too long to respond" warnings
      # that cause span drops and metric under-counting on busy nodes.
      OTEL_EBPF_CHANNEL_BUFFER_LEN: "500"
      # Reduce metrics export interval from default 60s to 15s.
      # Flushes histogram aggregations 4x more often, reducing backpressure
      # on the spanNameAggregatedMetrics pipeline stage.
      OTEL_EBPF_METRICS_INTERVAL: "15s"
      # Send eBPF events immediately instead of batching — reduces burst
      # size hitting the Go pipeline during high-throughput periods.
      OTEL_EBPF_BPF_HIGH_REQUEST_VOLUME: "true"
    containerSecurityContext: { }
    collector:
      enabled: true
      clickhouseExporter:
        enabled: false
      image:
        repository: otel/opentelemetry-collector-contrib
        tag: "0.145.0"
      resources:
        requests:
          memory: 64Mi
        limits:
          memory: 128Mi
      prometheusPort: 9400
      podMonitorLabels:
        release: kube-prometheus-stack

eventGenerator:
  enabled: false
  image:
    repository: kvisor-event-generator
    tag: latest

mockServer:
  enabled: true
  image:
    repository: kvisor-mock-server
    tag: latest

clickhouse:
  enabled: false
  persistentVolume:
    size: 10Gi

  auth:
    database: "kvisor"
    username: "kvisor"
    password: "kvisor"

